## Input data from filebeats which is installed on the webserver
input { beats {
        port => 5000
                codec => plain { charset => "UTF-8" }
        type => "log"
}
}

## Add your filters / logstash plugins configuration here
## Logs from flask webserver is used in the following case, a typical log line is given below as an example
## 2019-07-04 19:05:40,406 - werkzeug - INFO - 67.169.38.238 - - [04/Jul/2019 19:05:40] "GET /static/fonts/glyphicons-halflings-regular.ttf HTTP/1.1" 404 -
## 2019-07-04 21:22:33,204 - werkzeug - INFO - 209.17.96.226 - - [04/Jul/2019 21:22:33] "GET / HTTP/1.0" 200 -
## 'grok' to match and transform the log, 'mutate' to add ts(from previous parsed fields) and remove unneccesary fields, 'date' to replace @timestamp & change ts type to date (two methods both OK)
filter {
        grok {
                match => { "message" => "%{YEAR:yr}-%{MONTHNUM:mo}-%{MONTHDAY:day} %{TIME:time},%{INT:mills} - %{WORD:user} - %{WORD:level} - %{IPORHOST:clientip} - - \[%{MONTHDAY:day1}/%{MONTH:mo1}/%{YEAR:yr1} %{TIME:time1}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response}"}
        }
        mutate {
                add_field => { "ts" => "%{yr}-%{mo}-%{day}T%{time}.%{mills}Z" }
                remove_field => ["user", "day1", "mo1", "yr1", "time1", "yr", "mo", "day", "time", "mills"]
        }
  date {
    match => ["ts", "yyyy-MM-dd'T'HH:mm:ss'.'SSS'Z'"]
    target => "@timestamp"
  }
date {
        match => [ "ts", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss'.'SSS'Z'"]
        target => "ts"
}
}

## Output to both console stdout and elasticsearch
output {
   stdout { codec => rubydebug }
  elasticsearch {
    user => "elastic"
    password => "changeme"
    hosts => ["http://54.184.97.231:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}